{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pix2pix_Modu_Not_U_Net.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nyfGmU0FWjIE","colab_type":"text"},"cell_type":"markdown","source":["## 1. import library"]},{"metadata":{"id":"sJqpE1t_W9Vd","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import TensorFlow >= 1.10 and enable eager execution\n","import tensorflow as tf\n","tf.enable_eager_execution()\n","\n","import os\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import PIL\n","from IPython.display import clear_output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c0zlJPlhWl-i","colab_type":"text"},"cell_type":"markdown","source":["## 2. Data Load"]},{"metadata":{"id":"1waJET20XA8U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"4da3fd05-c53a-47c7-d99b-76877a4a914d","executionInfo":{"status":"ok","timestamp":1543967670583,"user_tz":-540,"elapsed":10393,"user":{"displayName":"junseop so","photoUrl":"","userId":"03070847090635331575"}}},"cell_type":"code","source":["path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n","                                      cache_subdir=os.path.abspath('.'),\n","                                      origin='https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz', \n","                                      extract=True)\n","\n","PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\n","30171136/30168306 [==============================] - 8s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"XRWbtCwLXBgZ","colab_type":"code","colab":{}},"cell_type":"code","source":["BUFFER_SIZE = 400\n","BATCH_SIZE = 1\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d-1ynlO5XDLC","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_image(image_file, is_train):\n","  image = tf.read_file(image_file)\n","  image = tf.image.decode_jpeg(image)\n","\n","  w = tf.shape(image)[1]\n","\n","  w = w // 2\n","  real_image = image[:, :w, :]\n","  input_image = image[:, w:, :]\n","\n","  input_image = tf.cast(input_image, tf.float32)\n","  real_image = tf.cast(real_image, tf.float32)\n","\n","  if is_train:\n","    # random jittering\n","    \n","    # resizing to 286 x 286 x 3\n","    input_image = tf.image.resize_images(input_image, [286, 286], \n","                                        align_corners=True, \n","                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    real_image = tf.image.resize_images(real_image, [286, 286], \n","                                        align_corners=True, \n","                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    \n","    # randomly cropping to 256 x 256 x 3\n","    stacked_image = tf.stack([input_image, real_image], axis=0)\n","    cropped_image = tf.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n","    input_image, real_image = cropped_image[0], cropped_image[1]\n","\n","    if np.random.random() > 0.5:\n","      # random mirroring\n","      input_image = tf.image.flip_left_right(input_image)\n","      real_image = tf.image.flip_left_right(real_image)\n","  else:\n","    input_image = tf.image.resize_images(input_image, size=[IMG_HEIGHT, IMG_WIDTH], \n","                                         align_corners=True, method=2)\n","    real_image = tf.image.resize_images(real_image, size=[IMG_HEIGHT, IMG_WIDTH], \n","                                        align_corners=True, method=2)\n","  \n","  # normalizing the images to [-1, 1]\n","  input_image = (input_image / 127.5) - 1\n","  real_image = (real_image / 127.5) - 1\n","\n","  return input_image, real_image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EW4J1myjXJXO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.map(lambda x: load_image(x, True))\n","train_dataset = train_dataset.batch(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hl69XxNjXNd6","colab_type":"code","colab":{}},"cell_type":"code","source":["test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n","test_dataset = test_dataset.map(lambda x: load_image(x, False))\n","test_dataset = test_dataset.batch(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SDMZA5PvXY2w","colab_type":"code","outputId":"3d3766fb-388e-40eb-c04e-0ac72c97ad32","executionInfo":{"status":"ok","timestamp":1543967671999,"user_tz":-540,"elapsed":11753,"user":{"displayName":"junseop so","photoUrl":"","userId":"03070847090635331575"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["train_dataset"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((?, 256, 256, 3), (?, 256, 256, 3)), types: (tf.float32, tf.float32)>"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"0_5XNIlsWpDS","colab_type":"text"},"cell_type":"markdown","source":["## 3. Build a model\n","\n","### Generator \n"]},{"metadata":{"id":"07ftUphajq-8","colab_type":"code","colab":{}},"cell_type":"code","source":["OUTPUT_CHANNELS = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P6KRxuTkXepc","colab_type":"code","colab":{}},"cell_type":"code","source":["class Downsample(tf.keras.Model):\n","  def __init__(self, filters, size, apply_batchnorm=True):\n","    super(Downsample, self).__init__()\n","    self.apply_batchnorm = apply_batchnorm\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.conv1 = tf.keras.layers.Conv2D(filters, (size,size),\n","                                       strides=2, padding='same',\n","                                       kernel_initializer=initializer,\n","                                       use_bias=False)\n","    if self.apply_batchnorm:\n","      self.batchnorm = tf.keras.layers.BatchNormalization()\n","      \n","  def call(self, x, training):\n","    x = self.conv1(x)\n","    if self.apply_batchnorm:\n","      x = self.batchnorm(x, training=training)\n","    x = tf.nn.leaky_relu(x)\n","    \n","    return x\n","  \n","  \n","class Upsample(tf.keras.Model):\n","  def __init__(self, filters, size, apply_dropout=False):\n","    super(Upsample, self).__init__()\n","    self.apply_dropout = apply_dropout\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.up_conv = tf.keras.layers.Conv2DTranspose(filters, (size,size),\n","                                                  strides=2,\n","                                                  padding='same',\n","                                                  kernel_initializer=initializer,\n","                                                  use_bias=False)\n","    self.batchnorm = tf.keras.layers.BatchNormalization()\n","    if self.apply_dropout:\n","      self.dropout = tf.keras.layers.Dropout(0.5)\n","      \n","  def call(self, x, training):\n","    x = self.up_conv(x)\n","    x = self.batchnorm(x, training=training)\n","    if self.apply_dropout:\n","      x = self.dropout(x, training=training)\n","    x = tf.nn.relu(x)\n","    \n","    return x\n","    \n","# Encoder  \n","class Generator(tf.keras.Model):\n","  \n","  def __init__(self):\n","    super(Generator, self).__init__()\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.down1 = Downsample(64, 4, apply_batchnorm=False)\n","    self.down2 = Downsample(128, 4)\n","    self.down3 = Downsample(256, 4)\n","    self.down4 = Downsample(512, 4)\n","    self.down5 = Downsample(512, 4)\n","    self.down6 = Downsample(512, 4)\n","    self.down7 = Downsample(512, 4)\n","    self.down8 = Downsample(512, 4)\n","    \n","    self.up1 = Upsample(512, 4, apply_dropout=True)\n","    self.up2 = Upsample(512, 4, apply_dropout=True)\n","    self.up3 = Upsample(512, 4, apply_dropout=True)\n","    self.up4 = Upsample(512, 4)\n","    self.up5 = Upsample(256, 4)\n","    self.up6 = Upsample(128, 4)\n","    self.up7 = Upsample(64, 4)\n","    \n","    self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS,\n","                                               (4,4), strides=2,\n","                                               padding='same',\n","                                               kernel_initializer=initializer)\n","        \n","      \n","  @tf.contrib.eager.defun  \n","  def __call__(self, x, training=True):\n","    # x (bs, 256, 256, 3)\n","    x = self.down1(x, training=training) # (bs, 128, 128, 64)\n","    x = self.down2(x, training=training) # (bs, 64, 64, 128)\n","    x = self.down3(x, training=training) # (bs, 32, 32, 256)\n","    x = self.down4(x, training=training) # (bs, 16, 16, 256)\n","    x = self.down5(x, training=training) # (bs, 8, 8, 512)\n","    x = self.down6(x, training=training) # (bs, 4, 4, 512)\n","    x = self.down7(x, training=training) # (bs, 2, 2, 512)\n","    x = self.down8(x, training=training) # (bs, 1, 1, 512)\n","    \n","    x = self.up1(x, training=training) # (bs, 2, 2, 1024) ??\n","    x = self.up2(x, training=training) # (bs, 4, 4, 1024)\n","    x = self.up3(x, training=training) # (bs, 8, 8, 1024)\n","    x = self.up4(x, training=training) # (bs, 16, 16, 1024)\n","    x = self.up5(x, training=training) # (bs, 32, 32, 512)\n","    x = self.up6(x, training=training) # (bs, 64, 64, 256)\n","    x = self.up7(x, training=training) # (bs, 128, 128, 128)\n","    \n","    x = self.last(x) # (bs, 256, 256, 3)\n","    x = tf.nn.tanh(x)\n","    \n","    return x\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1hvfm2iNX5Er","colab_type":"text"},"cell_type":"markdown","source":["### Discriminator"]},{"metadata":{"id":"Spz0sLr6X7aX","colab_type":"code","colab":{}},"cell_type":"code","source":["class DiscDownsample(tf.keras.Model):\n","  def __init__(self, filters, size, apply_batchnorm=True):\n","    super(DiscDownsample, self).__init__()\n","    self.apply_batchnorm = apply_batchnorm\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.conv1 = tf.keras.layers.Conv2D(filters, (size,size),\n","                                       strides=2, padding='same',\n","                                       kernel_initializer=initializer,\n","                                       use_bias=False)\n","    if self.apply_batchnorm:\n","      self.batchnorm = tf.keras.layers.BatchNormalization()\n","      \n","  def call(self, x, training):\n","    x = self.conv1(x)\n","    if self.apply_batchnorm:\n","      x = self.batchnorm(x, training=training)\n","    x = tf.nn.leaky_relu(x)\n","    \n","    return x\n","  \n","  \n","class Discriminator(tf.keras.Model):\n","\n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    self.down1 = DiscDownsample(64, 4, False)\n","    self.down2 = DiscDownsample(128, 4)\n","    self.down3 = DiscDownsample(256, 4)\n","    \n","    # we are zero padding here with 1 because we need our shape to \n","    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n","    self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n","    self.conv = tf.keras.layers.Conv2D(512, (4,4), strides=1,\n","                                      kernel_initializer=initializer,\n","                                      use_bias=False)\n","    self.batchnorm1 = tf.keras.layers.BatchNormalization()\n","    \n","    # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n","    self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n","    self.last = tf.keras.layers.Conv2D(1, (4,4), strides=1, \n","                                      kernel_initializer=initializer)\n","    \n","    \n","  @tf.contrib.eager.defun\n","  def call(self, inp, tar, training=True):\n","    # concatenating the input and the target\n","    x = tf.concat([inp, tar], axis=-1) # (bs, 256, 256, channels*2)\n","    x = self.down1(x, training=training) # (bs, 128, 128, 64)\n","    x = self.down2(x, training=training) # (bs, 64, 64, 128)\n","    x = self.down3(x, training=training) # (bs, 32, 32, 256)\n","    \n","    x = self.zero_pad1(x) # (bs, 34, 34, 256)\n","    x = self.conv(x) # (bs, 31, 31, 512)\n","    x = self.batchnorm1(x, training=training)\n","    x = tf.nn.leaky_relu(x)\n","    \n","    x = self.zero_pad2(x) # (bs, 33, 33, 512)\n","    # don't add a sigmoid activation here since\n","    # the loss function expects raw logits.\n","    x = self.last(x) # (bs, 30, 30, 1)\n","    \n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_xBXm21yj7n8","colab_type":"code","colab":{}},"cell_type":"code","source":["# The call function of Generator and Discriminator have been decorated\n","# with tf.contrib.eager.defun()\n","# We get a performance speedup if defun is used (~25 seconds per epoch)\n","generator = Generator()\n","discriminator = Discriminator()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KWA2QwkLX7kc","colab_type":"text"},"cell_type":"markdown","source":["### Build a loss"]},{"metadata":{"id":"KJeczUT1r0e3","colab_type":"code","colab":{}},"cell_type":"code","source":["LAMBDA = 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HD_qE2YzX97s","colab_type":"code","colab":{}},"cell_type":"code","source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","  real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_real_output),\n","                                             logits = disc_real_output)\n","  generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.zeros_like(disc_generated_output),\n","                                                  logits = disc_generated_output)\n","  \n","  total_disc_loss = real_loss + generated_loss\n","  \n","  return total_disc_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dCVzC1yvk9U4","colab_type":"code","colab":{}},"cell_type":"code","source":["def generator_loss(disc_generated_output, gen_output, target):\n","  gan_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_generated_output),\n","                                            logits = disc_generated_output)\n","  #mean absolute error\n","  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","  \n","  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","  \n","  return total_gen_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ekHbB0X0lPLf","colab_type":"code","colab":{}},"cell_type":"code","source":["discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\n","generator_optimizer = tf.train.AdamOptimizer(1e-4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3jJRoCggl2_7","colab_type":"text"},"cell_type":"markdown","source":["### Checkpoints"]},{"metadata":{"id":"jO66Jq6Ql5ef","colab_type":"code","colab":{}},"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AWqEa1Y4Wyiu","colab_type":"text"},"cell_type":"markdown","source":["## 4. Training (using Session)"]},{"metadata":{"id":"8dZnhP1CWoLp","colab_type":"code","colab":{}},"cell_type":"code","source":["EPOCHS = 200"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xAWHSdadWq8n","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_images(model, test_input, tar):\n","  prediction = model(test_input, training=True)\n","  plt.figure(figsize=(15,15))\n","\n","  display_list = [test_input[0], tar[0], prediction[0]]\n","  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","  for i in range(3):\n","    plt.subplot(1, 3, i+1)\n","    plt.title(title[i])\n","    plt.imshow(display_list[i] * 0.5 + 0.5)\n","    plt.axis('off')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9Crie3YRm9g_","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(dataset, epochs):  \n","  for epoch in range(epochs):\n","    start = time.time()\n","    \n","    for input_image, target in dataset:\n","      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        gen_output = generator(input_image, training=True)\n","        \n","        disc_real_output = discriminator(input_image, target, training=True)\n","        disc_generated_output = discriminator(input_image, gen_output, training=True)\n","        \n","        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n","        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","        \n","        generator_gradients = gen_tape.gradient(gen_loss, generator.variables)\n","        discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.variables)\n","        \n","        generator_optimizer.apply_gradients(zip(generator_gradients, generator.variables))\n","        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.variables))\n","\n","    if epoch % 1 == 0:\n","      clear_output(wait=True)\n","      for inp, tar in test_dataset.take(1):\n","        generate_images(generator, inp, tar)\n","        \n","    #saving checkpoints the model very 20 epochs\n","    if (epoch + 1) % 20 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","    \n","    print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time() - start))\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"UCCWk5Q1nESV","colab_type":"code","outputId":"cf6dfba2-05df-4753-fc01-5d00580a2188","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["train(train_dataset, EPOCHS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"metadata":{"id":"J2O3cdrrnJC3","colab_type":"text"},"cell_type":"markdown","source":["### Testing\n"]},{"metadata":{"id":"uDG2EWZjnGPD","colab_type":"code","colab":{}},"cell_type":"code","source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vQKQsghfnPiI","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# Run the trained model on the entire test dataset\n","for inp, tar in test_dataset:\n","  generate_images(generator, inp, tar)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y8erXMIsnQHb","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}